pip install gradio
import gradio as gr
import pandas as pd

!pip install transformers

# install fast_ml library
! pip install fast_ml --quiet

pip install emot

# imports
import pandas as pd
from fast_ml.model_development import train_valid_test_split


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import torch
import transformers as ppb
import warnings
warnings.filterwarnings('ignore')


import tensorflow as tf
import torch
import time
import datetime
import random
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

from google.colab import drive

import transformers
from transformers import BertTokenizer
from torch.utils.data import TensorDataset, random_split
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from transformers import BertForSequenceClassification, AdamW, BertConfig
from transformers import get_linear_schedule_with_warmup

from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder


import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np

from tabulate import tabulate
from tqdm import trange
import random

import json
import re
import numpy as np
import torch as th


### Install Hugging Face Hub
!python -m pip install huggingface_hub
!python -m pip install evaluate


from huggingface_hub import notebook_login

notebook_login()


pip install zemberek-python
!pip install zeyrek



import re
import time
import logging

from zemberek import (
    TurkishSpellChecker,
    TurkishSentenceNormalizer,
    TurkishSentenceExtractor,
    TurkishMorphology,
    TurkishTokenizer
)
logger = logging.getLogger(__name__)


import gradio as gr
import pandas as pd


import pandas as pd



#def auth(username, password):
    #if username == "ates" and password == "42":
     #   return True
    #else:
     #   return False





def auth(username, password):
    if username == "ates" and password == "42":
        return True
    else:
        return False


def predict(df):
# TODO:
    df["offansive"] = 1
    df["target"] = None

    # ***************************
    # WRITE YOUR INFERENCE STEPS BELOW
    #
    # 
    #
    # *********** END ***********
    return df


    def get_file(file):
        output_file = "output_TrendMiner.csv"

        # For windows users, replace path seperator
        file_name = teknofest_train_final.csv.replace("\\", "/")

        df = pd.read_csv(file_name, sep="|")

        predict(df)
        df.to_csv(output_file, index=False, sep="|")
        return (output_file)


# Launch the interface with user password
iface = gr.Interface(get_file, "file", "file")

if __name__ == "__main__":
        iface.launch(share=True, auth=auth)

    ##İstenmeyen karakterlerin temizliği başlangıç
    #import re
def cleanTxt(text):
        text=re.sub(r'@[A-Za-z0-9]+','',text)
        text=re.sub(r'#','',text)
        text=re.sub(r'RT[\s]+','',text)
        text=re.sub(r'https?:\/\/\S+_:','',text)
        return text
        df['text'] = df['text'].apply(cleanTxt)

    ###İstenmeyen karakterlerin temizliği Son

    #VERİ SETİNİ KATEGORİLERE AYIRMA BAŞLANGIÇ

    #df.loc[df['target']=='SEXIST','encoded_categories'] = '1'
    #df.loc[df['target']=='RACIST','encoded_categories'] = '2'
    #df.loc[df['target']=='PROFANITY','encoded_categories'] ='3'
    #df.loc[df['target']=='INSULT','encoded_categories'] = '4'
    #df.loc[df['target']=='OTHER','encoded_categories'] = '0'

    #df['encoded_categories'].fillna('Other', inplace=True)

    #VERİ SETİNİ KATEGORİLERE AYIRMA SON


    ###############################################################
    #df['encoded_categories'] = LabelEncoder().fit_transform(df['encoded_categories'])

    #print(df.sample(10))
    #print(df.groupby('target').size())

    #predict(df)
    #df.to_csv(output_file, index=False, sep="|")
    #return (output_file)

    #################################################################



    
df.columns = df.columns.str.replace('|', '')

df[['id','text','is_offensive','target']] = df.idtextis_offensivetarget.str.split("|",expand=True) 
df.drop('idtextis_offensivetarget', inplace=True, axis=1)
df.drop('id', inplace=True, axis=1)
df



df.loc[df['target']=='SEXIST','encoded_categories'] = '1'
df.loc[df['target']=='RACIST','encoded_categories'] = '2'
df.loc[df['target']=='PROFANITY','encoded_categories'] ='3'
df.loc[df['target']=='INSULT','encoded_categories'] = '4'
df.loc[df['target']=='OTHER','encoded_categories'] = '0'

df['encoded_categories'].fillna('Other', inplace=True)





df['encoded_categories'] = LabelEncoder().fit_transform(df['encoded_categories'])

print(df.sample(10))
print(df.groupby('target').size()


from huggingface_hub import notebook_login
notebook_login()

tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-cased', do_lower_case=True,truncation=True)
sentences = df.text.values
max_len = 20


tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-cased', do_lower_case=True,truncation=True)
sentences = df.text.values
max_len = 20
    
    ############################################ Bert Tokenizasyon Modeli Son

    ### Veri Seti Eğitim Test Ayrılması Başlangıç

training = df.groupby('target').apply(lambda x : x.sample(frac = 0.8))
test = pd.concat([df,training]).drop_duplicates(keep=False)

print("Training: ", len(training))
print("Test: ", len(test))

training_texts = training.text.values
training_labels = training.encoded_categories.values

    ################# Veri Seti Eğitim Test Ayrılması Son


    ##### Eğitim Ayarları Başlangıç

input_ids = []
attention_masks = []

for text in training_texts:
        encoded_dict = tokenizer.encode_plus(
                           text,
                            truncation=True,                     
                           add_special_tokens = True,
                           max_length = max_len,      
                           pad_to_max_length = True,
                           return_attention_mask = True, 
                           return_tensors = 'pt',
                      )
    
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(training_labels)

print('Original: ', training_texts[64])
print('Token IDs:', input_ids[64])

    ############## Eğitim Ayarları Son

    ##################### Modelin Eğitilmesi Başlangıç

train_dataset = TensorDataset(input_ids, attention_masks, labels)

batch_size = 16

train_dataloader = DataLoader(
            train_dataset,  
            sampler = RandomSampler(train_dataset), 
            batch_size = batch_size 
        )

number_of_categories = len(df['encoded_categories'].unique())

model = BertForSequenceClassification.from_pretrained(
    "dbmdz/bert-base-turkish-cased",
num_labels = number_of_categories, 
output_attentions = False,
output_hidden_states = False,
    )
 
model.cuda()

########################################### Modelin Eğitilmesi Son


############### Model Parametreleri Başlangıç

epochs = 4

optimizer = AdamW(model.parameters(),
                  lr = 5e-5,
                  eps = 1e-8 
                )

total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)
    
##################################### Model Parametreleri Son


########################################### Modelin Eğitilmesi Başlangıç

def format_time(elapsed):
    elapsed_rounded = int(round((elapsed)))
    return str(datetime.timedelta(seconds=elapsed_rounded))

    # This training code is based on the `run_glue.py` script here:
    # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128

seed_val = 1903

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

training_stats = []
total_t0 = time.time()

for epoch_i in range(0, epochs):
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    t0 = time.time()
    total_train_loss = 0
    model.train()

    for step, batch in enumerate(train_dataloader):
        if step % 10 == 0 and not step == 0:
            elapsed = format_time(time.time() - t0)
            print('Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        model.zero_grad()        
        output = model(b_input_ids, 
                       token_type_ids=None, 
                       attention_mask=b_input_mask, 
                       labels=b_labels)
        loss = output['loss']
        logits = output['logits']
        total_train_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

    avg_train_loss = total_train_loss / len(train_dataloader)            
    training_time = format_time(time.time() - t0)

    print("Average training loss: {0:.2f}".format(avg_train_loss))
    print("Training epoch took: {:}".format(training_time))

    training_stats.append(
               {
                'epoch': epoch_i + 1,
               'Training Loss': avg_train_loss,
               'Training Time': training_time,
                }
            )

print("Training completed in {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))

##################################################### Modelin Eğitilmesi Son


############# Training Loss Grafik Başlangıç

df_stats = pd.DataFrame(data=training_stats)
plt.plot(df_stats['Training Loss'], label="Training")
plt.title("Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.xticks([1, 2, 3, 4, 5 ])
plt.show()



######################################## Training Loss Grafik Son

#################### Tokenizasyon Ayarlar Başlangıç

test_texts = test.text.values
test_labels = test.encoded_categories.values

input_ids = []
attention_masks = []

for text in test_texts:
    encoded_dict = tokenizer.encode_plus(
                        text,                     
                        add_special_tokens = True, 
                        max_length = max_len,          
                        pad_to_max_length = True,
                        return_attention_mask = True,  
                        return_tensors = 'pt',   
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(test_labels)

batch_size = 15  

prediction_data = TensorDataset(input_ids, attention_masks, labels)
prediction_sampler = SequentialSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)

    ################################################ Tokenizasyon Ayarlar Son

    





#################### Modelin Test Verileri Üzerinde Tahmin Yapması Başlangıç

print('Prediction started on test data')
model.eval()
predictions , true_labels = [], []

for batch in prediction_dataloader:
  batch = tuple(t.to(device) for t in batch)
  b_input_ids, b_input_mask, b_labels = batch

  with torch.no_grad():
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()
  
  predictions.append(logits)
  true_labels.append(label_ids)

print('Prediction completed')

prediction_set = []

for i in range(len(true_labels)):
  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()
  prediction_set.append(pred_labels_i)

prediction_scores = [item for sublist in prediction_set for item in sublist]

############################# Modelin Test Verileri Üzerinde Tahmin Yapması Son

    
# Launch the interface with user password
iface = gr.Interface(get_file, "file", "file")
if __name__ == "__main__":
    iface.launch(share=True, auth=auth)
